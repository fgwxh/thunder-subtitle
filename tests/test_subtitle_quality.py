"""
å­—å¹•è´¨é‡è¯„ä¼°æµ‹è¯•è„šæœ¬ v2
æ”¹è¿›ç‰ˆï¼šæ›´å¥½åœ°å¤„ç†SRTå’ŒASSæ ¼å¼
"""
import asyncio
import sys
import os
import re
from collections import Counter
import math

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.thunder_subtitle_cli.client import ThunderClient


def extract_text_from_srt(content: str) -> str:
    """ä»SRTæ ¼å¼ä¸­æå–çº¯æ–‡æœ¬"""
    lines = content.split('\n')
    text_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        if line.isdigit():
            continue
        if '-->' in line:
            continue
        if re.match(r'^\d+$', line):
            continue
        text_lines.append(line)
    
    return '\n'.join(text_lines)


def extract_text_from_ass(content: str) -> str:
    """ä»ASSæ ¼å¼ä¸­æå–çº¯æ–‡æœ¬"""
    lines = content.split('\n')
    text_lines = []
    in_events = False
    
    for line in lines:
        line = line.strip()
        
        if line.startswith('[Events]'):
            in_events = True
            continue
        
        if line.startswith('[') and in_events:
            break
        
        if in_events and line.startswith('Dialogue:'):
            parts = line.split(',', 9)
            if len(parts) >= 10:
                text = parts[9]
                text = re.sub(r'\{[^}]*\}', '', text)
                text = re.sub(r'\\N', '\n', text)
                text = re.sub(r'\\n', '\n', text)
                text = text.strip()
                if text:
                    text_lines.append(text)
    
    return '\n'.join(text_lines)


def extract_text(content: str, ext: str) -> str:
    """æ ¹æ®æ‰©å±•åæå–æ–‡æœ¬"""
    ext = ext.lower().lstrip('.')
    if ext == 'ass':
        return extract_text_from_ass(content)
    else:
        return extract_text_from_srt(content)


def split_into_sentences(text: str) -> list:
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?\n]+', text)
    return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 1]


def calculate_perplexity(text: str) -> float:
    """
    è®¡ç®—å›°æƒ‘åº¦ï¼ˆåŸºäºN-gramï¼‰
    å›°æƒ‘åº¦è¶Šä½ï¼Œæ–‡æœ¬è¶Šè‡ªç„¶
    """
    if len(text) < 10:
        return 100.0
    
    bigram_counts = Counter()
    for i in range(len(text) - 1):
        bigram = text[i:i+2]
        bigram_counts[bigram] += 1
    
    total_bigrams = sum(bigram_counts.values())
    if total_bigrams == 0:
        return 100.0
    
    perplexity = 0
    for count in bigram_counts.values():
        prob = count / total_bigrams
        perplexity -= prob * math.log2(prob)
    
    return round(2 ** perplexity, 2)


def analyze_fluency(text: str) -> dict:
    """åˆ†ææ–‡æœ¬æµç•…åº¦"""
    sentences = split_into_sentences(text)
    
    if not sentences:
        return {
            'fluency_score': 0,
            'sentence_count': 0,
            'avg_sentence_length': 0,
            'vocabulary_richness': 0
        }
    
    sentence_lengths = [len(s) for s in sentences]
    avg_length = sum(sentence_lengths) / len(sentence_lengths)
    
    chars = [c for c in text if not c.isspace()]
    unique_chars = len(set(chars))
    total_chars = len(chars)
    vocabulary_richness = unique_chars / total_chars if total_chars > 0 else 0
    
    length_variance = 0
    if len(sentence_lengths) > 1:
        mean = avg_length
        length_variance = sum((l - mean) ** 2 for l in sentence_lengths) / len(sentence_lengths)
        length_variance = math.sqrt(length_variance)
    
    ideal_avg_length = 15
    length_score = max(0, 100 - abs(avg_length - ideal_avg_length) * 3)
    
    variance_score = max(0, 100 - length_variance * 2)
    
    fluency_score = (
        length_score * 0.3 +
        variance_score * 0.3 +
        vocabulary_richness * 100 * 0.2 +
        min(len(sentences) / 50 * 100, 100) * 0.2
    )
    
    return {
        'fluency_score': round(fluency_score, 2),
        'sentence_count': len(sentences),
        'avg_sentence_length': round(avg_length, 2),
        'length_std': round(length_variance, 2),
        'vocabulary_richness': round(vocabulary_richness, 4)
    }


def detect_machine_translation(text: str) -> dict:
    """æ£€æµ‹æœºå™¨ç¿»è¯‘ç‰¹å¾"""
    mt_indicators = []
    score = 0
    
    mt_patterns = [
        (r'çš„{3,}', 'è¿ç»­"çš„"', 15),
        (r'äº†{3,}', 'è¿ç»­"äº†"', 15),
        (r'æ˜¯{3,}', 'è¿ç»­"æ˜¯"', 15),
        (r'æˆ‘æˆ‘æˆ‘', 'é‡å¤ä»£è¯', 20),
        (r'ä½ ä½ ä½ ', 'é‡å¤ä»£è¯', 20),
        (r'ä»–ä»–ä»–', 'é‡å¤ä»£è¯', 20),
        (r'[ï¼Œã€‚ã€]{2,}', 'è¿ç»­æ ‡ç‚¹', 10),
        (r'\s{4,}', 'è¿‡å¤šç©ºæ ¼', 5),
    ]
    
    for pattern, desc, penalty in mt_patterns:
        matches = re.findall(pattern, text)
        if matches:
            mt_indicators.append(f"{desc}: {len(matches)}æ¬¡")
            score += penalty * len(matches)
    
    unnatural_phrases = [
        'æ‰“å¼€ç¯', 'å…³é—­ç¯', 'æ‰“å¼€é—¨', 'å…³é—­é—¨',
        'è¿™æ˜¯éå¸¸', 'é‚£æ˜¯éå¸¸', 'å®ƒæ˜¯å¾ˆ',
        'åœ¨è¿™ä¸€ç‚¹ä¸Š', 'åœ¨æŸç§ç¨‹åº¦ä¸Š',
        'è¯·è®©æˆ‘', 'è¯·ç»™æˆ‘',
    ]
    
    found_unnatural = []
    for phrase in unnatural_phrases:
        count = text.count(phrase)
        if count > 0:
            found_unnatural.append(f"{phrase}: {count}æ¬¡")
            score += 10 * count
    
    mt_probability = min(score / 100, 1.0)
    
    return {
        'mt_probability': round(mt_probability, 2),
        'mt_indicators': mt_indicators,
        'unnatural_phrases': found_unnatural
    }


def analyze_punctuation(text: str) -> dict:
    """åˆ†ææ ‡ç‚¹ç¬¦å·ä½¿ç”¨"""
    chinese_punct = r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘â€¦â€”]'
    english_punct = r'[,.!?;:\"\'()\[\]]'
    
    cn_count = len(re.findall(chinese_punct, text))
    en_count = len(re.findall(english_punct, text))
    
    chars_no_space = len([c for c in text if not c.isspace()])
    
    total_punct = cn_count + en_count
    punct_ratio = total_punct / chars_no_space if chars_no_space > 0 else 0
    
    normal_ratio = 0.02 <= punct_ratio <= 0.10
    
    return {
        'chinese_punctuation': cn_count,
        'english_punctuation': en_count,
        'punctuation_ratio': round(punct_ratio, 4),
        'normal_ratio': normal_ratio
    }


def analyze_content_quality(text: str) -> dict:
    """åˆ†æå†…å®¹è´¨é‡"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len([c for c in text if not c.isspace()])
    
    chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
    
    sentences = split_into_sentences(text)
    
    short_sentences = sum(1 for s in sentences if len(s) < 5)
    long_sentences = sum(1 for s in sentences if len(s) > 50)
    
    return {
        'chinese_ratio': round(chinese_ratio, 4),
        'total_chars': total_chars,
        'short_sentence_ratio': round(short_sentences / len(sentences), 2) if sentences else 0,
        'long_sentence_ratio': round(long_sentences / len(sentences), 2) if sentences else 0
    }


def calculate_overall_quality(subtitle_content: str, ext: str) -> dict:
    """ç»¼åˆè¯„ä¼°å­—å¹•è´¨é‡"""
    text = extract_text(subtitle_content, ext)
    
    if not text or len(text) < 10:
        return {'error': 'æ–‡æœ¬å†…å®¹å¤ªå°‘', 'overall_score': 0, 'quality_level': 'âŒ æ— æ•ˆ'}
    
    perplexity = calculate_perplexity(text)
    fluency = analyze_fluency(text)
    mt_detection = detect_machine_translation(text)
    punctuation = analyze_punctuation(text)
    content = analyze_content_quality(text)
    
    perplexity_score = max(0, 100 - perplexity)
    
    quality_score = (
        perplexity_score * 0.15 +
        fluency['fluency_score'] * 0.35 +
        (1 - mt_detection['mt_probability']) * 100 * 0.25 +
        (100 if punctuation['normal_ratio'] else 50) * 0.1 +
        content['chinese_ratio'] * 100 * 0.15
    )
    
    if quality_score >= 70:
        quality_level = 'ğŸŸ¢ ä¼˜è´¨'
    elif quality_score >= 50:
        quality_level = 'ğŸŸ¡ ä¸€èˆ¬'
    elif quality_score >= 30:
        quality_level = 'ğŸŸ  è¾ƒå·®'
    else:
        quality_level = 'ğŸ”´ å¾ˆå·®'
    
    return {
        'text_length': len(text),
        'perplexity': perplexity,
        'fluency': fluency,
        'mt_detection': mt_detection,
        'punctuation': punctuation,
        'content': content,
        'overall_score': round(quality_score, 2),
        'quality_level': quality_level
    }


async def test_subtitle_quality():
    """æµ‹è¯•å­—å¹•è´¨é‡è¯„ä¼°"""
    print("=" * 70)
    print("å­—å¹•è´¨é‡è¯„ä¼°æµ‹è¯• v2 - IPX-580")
    print("=" * 70)
    
    client = ThunderClient()
    
    print("\næ­£åœ¨æœç´¢å­—å¹•...")
    results = await client.search(query="ipx580")
    
    if not results:
        print("æœªæ‰¾åˆ°å­—å¹•")
        return
    
    print(f"æ‰¾åˆ° {len(results)} ä¸ªå­—å¹•\n")
    
    all_results = []
    
    for i, subtitle in enumerate(results[:10]):
        print(f"\n{'='*70}")
        print(f"å­—å¹• #{i+1}: {subtitle.name}")
        print(f"è¯­è¨€: {', '.join(subtitle.languages) if subtitle.languages else 'æœªçŸ¥'}")
        print(f"æ‰©å±•å: {subtitle.ext}")
        print("-" * 70)
        
        try:
            print("æ­£åœ¨ä¸‹è½½å­—å¹•å†…å®¹...")
            content_bytes = await client.download_bytes(url=subtitle.url, timeout_s=30)
            
            try:
                content = content_bytes.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    content = content_bytes.decode('gbk')
                except UnicodeDecodeError:
                    content = content_bytes.decode('utf-8', errors='ignore')
            
            text = extract_text(content, subtitle.ext)
            
            print(f"\næå–çš„æ–‡æœ¬é¢„è§ˆ (å‰300å­—ç¬¦):")
            print("-" * 50)
            print(text[:300])
            print("-" * 50)
            
            quality = calculate_overall_quality(content, subtitle.ext)
            
            print(f"\nğŸ“Š è´¨é‡è¯„ä¼°ç»“æœ:")
            print(f"  æ–‡æœ¬é•¿åº¦: {quality.get('text_length', 0)} å­—ç¬¦")
            print(f"  å›°æƒ‘åº¦: {quality.get('perplexity', 0)} (è¶Šä½è¶Šå¥½)")
            
            fluency = quality.get('fluency', {})
            print(f"  æµç•…åº¦: {fluency.get('fluency_score', 0)} åˆ†")
            print(f"    - å¥å­æ•°: {fluency.get('sentence_count', 0)}")
            print(f"    - å¹³å‡å¥é•¿: {fluency.get('avg_sentence_length', 0)} å­—")
            print(f"    - è¯æ±‡ä¸°å¯Œåº¦: {fluency.get('vocabulary_richness', 0)}")
            
            mt = quality.get('mt_detection', {})
            print(f"  æœºå™¨ç¿»è¯‘æ¦‚ç‡: {mt.get('mt_probability', 0):.0%}")
            if mt.get('mt_indicators'):
                print(f"    - MTç‰¹å¾: {', '.join(mt['mt_indicators'][:3])}")
            if mt.get('unnatural_phrases'):
                print(f"    - ä¸è‡ªç„¶çŸ­è¯­: {', '.join(mt['unnatural_phrases'][:3])}")
            
            punct = quality.get('punctuation', {})
            print(f"  æ ‡ç‚¹ç¬¦å·: ä¸­æ–‡{punct.get('chinese_punctuation', 0)}/è‹±æ–‡{punct.get('english_punctuation', 0)}")
            print(f"    - æ¯”ä¾‹: {punct.get('punctuation_ratio', 0):.2%}")
            print(f"    - æ­£å¸¸: {'æ˜¯' if punct.get('normal_ratio') else 'å¦'}")
            
            content_info = quality.get('content', {})
            print(f"  ä¸­æ–‡æ¯”ä¾‹: {content_info.get('chinese_ratio', 0):.1%}")
            
            print(f"\n  â˜…â˜…â˜… ç»¼åˆè¯„åˆ†: {quality.get('overall_score', 0)} åˆ†")
            print(f"  â˜…â˜…â˜… è´¨é‡ç­‰çº§: {quality.get('quality_level', 'æœªçŸ¥')}")
            
            all_results.append({
                'name': subtitle.name,
                'ext': subtitle.ext,
                'score': quality.get('overall_score', 0),
                'level': quality.get('quality_level', 'æœªçŸ¥'),
                'mt_prob': mt.get('mt_probability', 0)
            })
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½æˆ–åˆ†æå¤±è´¥: {e}")
            all_results.append({
                'name': subtitle.name,
                'ext': subtitle.ext,
                'score': 0,
                'level': 'âŒ é”™è¯¯',
                'mt_prob': 0
            })
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ è¯„åˆ†æ±‡æ€» (æŒ‰è¯„åˆ†æ’åº)")
    print("=" * 70)
    
    all_results.sort(key=lambda x: x['score'], reverse=True)
    
    print(f"{'æ’å':<4} {'è¯„åˆ†':<8} {'MTæ¦‚ç‡':<8} {'ç­‰çº§':<10} {'æ–‡ä»¶å'}")
    print("-" * 70)
    for i, r in enumerate(all_results):
        print(f"{i+1:<4} {r['score']:<8.1f} {r['mt_prob']:<8.0%} {r['level']:<10} {r['name'][:40]}")
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(test_subtitle_quality())
